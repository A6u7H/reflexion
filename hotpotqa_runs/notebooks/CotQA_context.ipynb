{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook for running Chain-of-Thought with supporting context experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "root = '../root/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from llm import AnyOpenAILLM\n",
    "from agents import CoTAgent, ReflexionStrategy\n",
    "from util import summarize_trial, log_trial, save_agents, save_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the HotPotQA Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotpot = joblib.load('../data/hotpot-qa-distractor-sample.joblib').reset_index(drop = True)\n",
    "\n",
    "hotpot['supporting_paragraphs'] = None\n",
    "for ind, row in hotpot.iterrows():\n",
    "    supporting_articles = row['supporting_facts']['title']\n",
    "    articles = row['context']['title']\n",
    "    sentences = row['context']['sentences'] \n",
    "    supporting_paragraphs = []\n",
    "    for article in supporting_articles:\n",
    "        supporting_paragraph = ''.join(sentences[np.where(articles == article)][0])\n",
    "        supporting_paragraphs.append(supporting_paragraph)\n",
    "    supporting_paragraphs = '\\n\\n'.join(supporting_paragraphs)\n",
    "    hotpot.at[ind, 'supporting_paragraphs'] = supporting_paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Reflexion Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    NONE: No reflection\n",
      "    LAST_ATTEMPT: Use last reasoning trace in context \n",
      "    REFLEXION: Apply reflexion to the next reasoning trace \n",
      "    LAST_ATTEMPT_AND_REFLEXION: Use last reasoning trace in context and apply reflexion to the next reasoning trace \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(ReflexionStrategy.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy: ReflexionStrategy = ReflexionStrategy.REFLEXION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_reflect_llm = \"gpt-4\"\n",
    "action_llm = \"gpt-4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize a CoTAgent for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import cot_agent_prompt, cot_reflect_agent_prompt, cot_reflect_prompt\n",
    "from fewshots import COT, COT_REFLECT\n",
    "agents = [CoTAgent(row['question'],\n",
    "                   row['supporting_paragraphs'],\n",
    "                   row['answer'],\n",
    "                   agent_prompt=cot_agent_prompt if strategy == ReflexionStrategy.NONE else cot_reflect_agent_prompt,\n",
    "                   cot_examples=COT,\n",
    "                   reflect_prompt=cot_reflect_prompt,\n",
    "                   reflect_examples=COT_REFLECT,\n",
    "                   self_reflect_llm= AnyOpenAILLM(\n",
    "                                            temperature=0,\n",
    "                                            max_tokens=250,\n",
    "                                            model_name=self_reflect_llm,\n",
    "                                            model_kwargs={\"stop\": \"\\n\"},\n",
    "                                            openai_api_key=os.environ['OPENAI_API_KEY']),\n",
    "                    action_llm= AnyOpenAILLM(\n",
    "                                            temperature=0,\n",
    "                                            max_tokens=250,\n",
    "                                            model_name=action_llm,\n",
    "                                            model_kwargs={\"stop\": \"\\n\"},\n",
    "                                            openai_api_key=os.environ['OPENAI_API_KEY']),\n",
    "                    ) for _, row in hotpot.iterrows()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run `n` trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 9 \n",
    "trial = 5 \n",
    "log = ''\n",
    "results = []\n",
    "\n",
    "llm = 'gpt4'\n",
    "run_dir = os.path.join(root, 'CoT', 'context', strategy.value + f'_{llm}')\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Basic config\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(run_dir, f'{n}_{trial}.log')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Load agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "agents_dir = os.path.join(run_dir, 'agents')\n",
    "for file in os.listdir(agents_dir):\n",
    "    agents.append(joblib.load(os.path.join(agents_dir, file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 22 November\n",
      "Answer: Roman\n",
      "Answer: in the village of Aldenham\n",
      "Answer: author\n",
      "Answer: a failed coup attempt\n",
      "Answer: novelist\n",
      "Answer: California\n",
      "Answer: super-regional shopping mall\n",
      "Answer: singer, songwriter\n",
      "Answer: German\n",
      "Answer: the port of Baltimore west to Sandy Hook\n",
      "Answer: New York\n",
      "Answer: Frederick Alexander\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid action: No, only Affiliated Managers Group.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: no\n",
      "Answer: cleaning, catering and security\n",
      "Answer: chronological collection of critical quotations\n",
      "Answer: The Bad Hemingway Contest\n",
      "Answer: \"Now and Then\" (1995)\n",
      "Answer: no\n",
      "Answer: the Cold War (1947–91)\n",
      "Answer: fortnightly women interest magazine\n",
      "Answer: 2 March 1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid action: Upon further research, I have confirmed that Larnelle Steward Harris was indeed born in the month of July. Therefore, the answer to the question is Yes, David Huntsinger has worked with a gospel singer born in the month of July.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Larnelle Harris\n",
      "Answer: The Bears\n",
      "Answer: Vivendi S.A.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid action: No, only Maxillaria is a genus of orchids.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: no\n",
      "Answer: fictional character\n",
      "Answer: 10 January 1920\n",
      "Answer: Ricard Rubio i Vives\n",
      "Finished Trial 6, Correct: 71, Incorrect: 26\n",
      "Answer: 22 November\n",
      "Answer: Roman\n",
      "Answer: in the village of Aldenham\n",
      "Answer: author\n",
      "Answer: a failed coup attempt\n",
      "Answer: novelist\n",
      "Answer: California\n",
      "Answer: super-regional shopping mall\n",
      "Answer: singer, songwriter\n",
      "Answer: German\n",
      "Answer: the port of Baltimore west to Sandy Hook\n",
      "Answer: New York\n",
      "Answer: Frederick Alexander\n",
      "Answer: no\n",
      "Answer: cleaning, catering and security\n",
      "Answer: chronological collection of critical quotations\n",
      "Answer: The Bad Hemingway Contest\n",
      "Answer: \"Now and Then\" (1995)\n",
      "Answer: no\n",
      "Answer: the Cold War (1947–91)\n",
      "Answer: fortnightly women interest magazine\n",
      "Answer: 2 March 1972\n",
      "Answer: Larnelle Harris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid action: The context does not provide information about the mascot of Mercer University.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The Bears\n",
      "Answer: Vivendi S.A.\n",
      "Answer: no\n",
      "Answer: fictional character\n",
      "Answer: 10 January 1920\n",
      "Answer: Ricard Rubio i Vives\n",
      "Finished Trial 7, Correct: 71, Incorrect: 28\n",
      "Answer: 22 November\n",
      "Answer: Roman\n",
      "Answer: in the village of Aldenham\n",
      "Answer: author\n",
      "Answer: a failed coup attempt\n",
      "Answer: novelist\n",
      "Answer: California\n",
      "Answer: super-regional shopping mall\n",
      "Answer: singer, songwriter\n",
      "Answer: German\n",
      "Answer: the port of Baltimore west to Sandy Hook\n",
      "Answer: New York\n",
      "Answer: Frederick Alexander\n",
      "Answer: no\n",
      "Answer: cleaning, catering and security\n",
      "Answer: chronological collection of critical quotations\n",
      "Answer: The Bad Hemingway Contest\n",
      "Answer: \"Now and Then\" (1995)\n",
      "Answer: no\n",
      "Answer: the Cold War (1947–91)\n",
      "Answer: fortnightly women interest magazine\n",
      "Answer: 2 March 1972\n",
      "Answer: Larnelle Harris\n",
      "Answer: The Bears\n",
      "Answer: Vivendi S.A.\n",
      "Answer: no\n",
      "Answer: fictional character\n",
      "Answer: 10 January 1920\n",
      "Answer: Ricard Rubio i Vives\n",
      "Finished Trial 8, Correct: 71, Incorrect: 29\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for agent in [a for a in agents if not a.is_correct()]:\n",
    "        agent.run(reflexion_strategy = strategy)\n",
    "        print(f'Answer: {agent.key}')\n",
    "    trial += 1\n",
    "    log += log_trial(agents, trial)\n",
    "    correct, incorrect = summarize_trial(agents)\n",
    "    results.append({'trial': trial, 'correct': len(correct), 'incorrect': len(incorrect)})\n",
    "    save_results(agents, results, run_dir)\n",
    "    print(f'Finished Trial {trial}, Correct: {len(correct)}, Incorrect: {len(incorrect)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving agent 0...\n",
      "Saving agent 1...\n",
      "Saving agent 2...\n",
      "Saving agent 3...\n",
      "Saving agent 4...\n",
      "Saving agent 5...\n",
      "Saving agent 6...\n",
      "Saving agent 7...\n",
      "Saving agent 8...\n",
      "Saving agent 9...\n",
      "Saving agent 10...\n",
      "Saving agent 11...\n",
      "Saving agent 12...\n",
      "Saving agent 13...\n",
      "Saving agent 14...\n",
      "Saving agent 15...\n",
      "Saving agent 16...\n",
      "Saving agent 17...\n",
      "Saving agent 18...\n",
      "Saving agent 19...\n",
      "Saving agent 20...\n",
      "Saving agent 21...\n",
      "Saving agent 22...\n",
      "Saving agent 23...\n",
      "Saving agent 24...\n",
      "Saving agent 25...\n",
      "Saving agent 26...\n",
      "Saving agent 27...\n",
      "Saving agent 28...\n",
      "Saving agent 29...\n",
      "Saving agent 30...\n",
      "Saving agent 31...\n",
      "Saving agent 32...\n",
      "Saving agent 33...\n",
      "Saving agent 34...\n",
      "Saving agent 35...\n",
      "Saving agent 36...\n",
      "Saving agent 37...\n",
      "Saving agent 38...\n",
      "Saving agent 39...\n",
      "Saving agent 40...\n",
      "Saving agent 41...\n",
      "Saving agent 42...\n",
      "Saving agent 43...\n",
      "Saving agent 44...\n",
      "Saving agent 45...\n",
      "Saving agent 46...\n",
      "Saving agent 47...\n",
      "Saving agent 48...\n",
      "Saving agent 49...\n",
      "Saving agent 50...\n",
      "Saving agent 51...\n",
      "Saving agent 52...\n",
      "Saving agent 53...\n",
      "Saving agent 54...\n",
      "Saving agent 55...\n",
      "Saving agent 56...\n",
      "Saving agent 57...\n",
      "Saving agent 58...\n",
      "Saving agent 59...\n",
      "Saving agent 60...\n",
      "Saving agent 61...\n",
      "Saving agent 62...\n",
      "Saving agent 63...\n",
      "Saving agent 64...\n",
      "Saving agent 65...\n",
      "Saving agent 66...\n",
      "Saving agent 67...\n",
      "Saving agent 68...\n",
      "Saving agent 69...\n",
      "Saving agent 70...\n",
      "Saving agent 71...\n",
      "Saving agent 72...\n",
      "Saving agent 73...\n",
      "Saving agent 74...\n",
      "Saving agent 75...\n",
      "Saving agent 76...\n",
      "Saving agent 77...\n",
      "Saving agent 78...\n",
      "Saving agent 79...\n",
      "Saving agent 80...\n",
      "Saving agent 81...\n",
      "Saving agent 82...\n",
      "Saving agent 83...\n",
      "Saving agent 84...\n",
      "Saving agent 85...\n",
      "Saving agent 86...\n",
      "Saving agent 87...\n",
      "Saving agent 88...\n",
      "Saving agent 89...\n",
      "Saving agent 90...\n",
      "Saving agent 91...\n",
      "Saving agent 92...\n",
      "Saving agent 93...\n",
      "Saving agent 94...\n",
      "Saving agent 95...\n",
      "Saving agent 96...\n",
      "Saving agent 97...\n",
      "Saving agent 98...\n",
      "Saving agent 99...\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(run_dir, f'{len(agents)}_questions_{trial}_trials.txt'), 'w') as f:\n",
    "    f.write(log)\n",
    "save_agents(agents, os.path.join(run_dir, 'agents'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e23f799cbd2581634725fbf6ce3480ae26192d78438dfafc8efe944acd6490d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
